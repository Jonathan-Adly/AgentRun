{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgentRun Usage \n",
    "\n",
    "Now, let's see AgentRun in action with something more complicated. We will take advantage of function calling and AgentRun, to have LLMs write and execute code on the fly to solve arbitrary tasks. You can find the full code under `docs/examples/`\n",
    "\n",
    "> We are using the REST API as it is recommend to seperate the code execution service from the rest of our infrastructure.\n",
    "\n",
    "First, we will install the needed packages. We are using mixtral here via groq to keep things fast and with minimal depenencies, but AgentRun works with any LLM out of the box. All what's required is for the LLM to return a code snippet.\n",
    "\n",
    "> FYI: OpenAI assistant tool ` code_interpreter` can execute code. Agentrun is a transparent, open-source version that can work with any LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests) (2024.2.2)\n",
      "Collecting groq\n",
      "  Using cached groq-0.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from groq)\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from groq)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from groq)\n",
      "  Using cached pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\n",
      "Collecting sniffio (from groq)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing-extensions<5,>=4.7 (from groq)\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.1 (from pydantic<3,>=1.9.0->groq)\n",
      "  Downloading pydantic_core-2.18.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Using cached groq-0.5.0-py3-none-any.whl (75 kB)\n",
      "Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached pydantic-2.7.0-py3-none-any.whl (407 kB)\n",
      "Downloading pydantic_core-2.18.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: typing-extensions, sniffio, h11, distro, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, groq\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.3.0 distro-1.9.0 groq-0.5.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 pydantic-2.7.0 pydantic-core-2.18.1 sniffio-1.3.1 typing-extensions-4.11.0\n"
     ]
    }
   ],
   "source": [
    "# Agentrun Example\n",
    "!pip install requests\n",
    "!pip install groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will setup a function that executed the code and returns an output. We are using the API here, so make sure to have it running before trying this. \n",
    "\n",
    "Here is the steps to run the API:\n",
    "1. git clone https://github.com/Jonathan-Adly/agentrun\n",
    "2. cd agentrun/agentrun-api\n",
    "3. cp .env.example .env.dev\n",
    "4. docker-compose up -d --build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "def execute_python_code(code: str) -> str:\n",
    "    response = requests.post(\"http://localhost:8000/v1/run/\", json={\"code\": code})\n",
    "    output = response.json()[\"output\"]\n",
    "    return output\n",
    "\n",
    "execute_python_code(\"print('Hello, World!')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will setup our LLM function calling skeleton code. We need:\n",
    "\n",
    "1. An LLM client such Groq or OpenAI or Anthropic (alternatively, you can use liteLLm as wrapper)\n",
    "2. The model you will use \n",
    "3. Our code execution tool - that encourages the LLM model to send us python code to execute reliably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import json\n",
    "\n",
    "client = Groq(api_key =\"Your API Key\")\n",
    "\n",
    "MODEL = 'mixtral-8x7b-32768'\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"execute_python_code\",\n",
    "            \"description\": \"Sends a python code snippet to the code execution environment and returns the output. The code execution environment can automatically import any library or package by importing.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"code\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The code snippet to execute. Must be a valid python code. Must use print() to output the result.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"code\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will setup a function to call our LLM of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_request(messages, tools=None, tool_choice=\"auto\", model=MODEL):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will set up a function that takes the user query and returns an answer. Using Agentrun to execute code when the LLM determines code execution is necesary to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query):\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\\n \n",
    "            Use the execute_python_code tool to run code if a question is better solved with code. You can use any package in the code snippet by simply importing. Like `import requests` would work fine.\\n\n",
    "            \"\"\",\n",
    "        }\n",
    "    )\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    chat_response = chat_completion_request(messages, tools=tools)\n",
    "\n",
    "    message = chat_response.choices[0].message\n",
    "    # tool call versus content\n",
    "    if message.tool_calls:\n",
    "        tool_call = message.tool_calls[0]\n",
    "        arg = json.loads(tool_call.function.arguments)[\"code\"]\n",
    "        print(f\"Executing code: {arg}\")\n",
    "        answer = execute_python_code(arg)\n",
    "        # Optional: call an LLM again to turn the answer to a human friendly response\n",
    "        query = \"Help translate the code output to a human friendly response. This was the user query: \" + query + \" The code output is: \" + answer\n",
    "        answer = get_answer(query)\n",
    "    else:\n",
    "        answer = message.content\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing code: import yfinance as yf\n",
      "\n",
      "stock = yf.Ticker('AAPL')\n",
      "historical_data = stock.history(period='3d')\n",
      "\n",
      "# Calculate the daily move as the percentage change from close to close\n",
      "daily_moves = [(historical_data['Close'][i] - historical_data['Close'][i-1]) / historical_data['Close'][i-1] for i in range(1, len(historical_data['Close']))]\n",
      "\n",
      "# Calculate the average daily move\n",
      "average_daily_move = sum(daily_moves) / len(daily_moves) * 100\n",
      "\n",
      "print(average_daily_move)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The average daily movement of Apple's stock in the last 3 days is approximately $2.60.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(\"what's the average daily move of Apple stock in the last 3 days?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "run-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
